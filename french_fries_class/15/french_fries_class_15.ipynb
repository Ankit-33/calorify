{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmfn2iWvMpRB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk3NPg-KM0Vn",
        "outputId": "6caa703c-a9a8-4a1a-d364-d447f895bcd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCwSr1-Yc0Mh"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/drive/MyDrive/data/train'\n",
        "validation_dir = '/content/drive/MyDrive/data/validation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqMEIBRKXUgm",
        "outputId": "4e9b64d1-c7c5-46bd-b7a4-dff1344c8416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2410 images belonging to 3 classes.\n",
            "Found 600 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 2s/step - accuracy: 0.3481 - loss: 1.2425 - val_accuracy: 0.4497 - val_loss: 1.0587\n",
            "Epoch 2/15\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4688 - loss: 1.0392 - val_accuracy: 0.6250 - val_loss: 0.9827\n",
            "Epoch 3/15\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 2s/step - accuracy: 0.4722 - loss: 1.0253 - val_accuracy: 0.4931 - val_loss: 0.9507\n",
            "Epoch 4/15\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3125 - loss: 1.1750 - val_accuracy: 0.6250 - val_loss: 0.7870\n",
            "Epoch 5/15\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2s/step - accuracy: 0.5680 - loss: 0.9250 - val_accuracy: 0.6181 - val_loss: 0.7974\n",
            "Epoch 6/15\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6250 - loss: 0.8302 - val_accuracy: 0.5000 - val_loss: 0.9676\n",
            "Epoch 7/15\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 2s/step - accuracy: 0.5823 - loss: 0.8566 - val_accuracy: 0.6736 - val_loss: 0.7378\n",
            "Epoch 8/15\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 109ms/step - accuracy: 0.7812 - loss: 0.6156 - val_accuracy: 0.3750 - val_loss: 1.0081\n",
            "Epoch 9/15\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 2s/step - accuracy: 0.6244 - loss: 0.8147 - val_accuracy: 0.6545 - val_loss: 0.8186\n",
            "Epoch 10/15\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6875 - loss: 0.6495 - val_accuracy: 0.7917 - val_loss: 0.6830\n",
            "Epoch 11/15\n",
            "\u001b[1m11/75\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 2s/step - accuracy: 0.7155 - loss: 0.6504"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from PIL import Image\n",
        "\n",
        "# Step 1: Prepare the Dataset Paths\n",
        "train_dir = '/content/drive/MyDrive/data/train'\n",
        "validation_dir = '/content/drive/MyDrive/data/validation'\n",
        "\n",
        "# Step 2: Define Image Parameters\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Step 3: Data Augmentation for Training Data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Step 4: Data Preprocessing for Validation Data\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Step 5: Load Training and Validation Data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Step 6: Build the CNN Model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')  # 3 classes: french_fries, samosa, pizza\n",
        "])\n",
        "\n",
        "# Step 7: Compile the Model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.save(\"CNN_Model.h5\")\n",
        "\n",
        "# Step 8: Train the Model\n",
        "EPOCHS = 15\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Step 9: Function to Fetch Images Using Google Custom Search API\n",
        "def fetch_images(query, num_images=1, api_key='YOUR_API_KEY', search_engine_id='YOUR_SEARCH_ENGINE_ID'):\n",
        "    search_url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'cx': search_engine_id,\n",
        "        'key': api_key,\n",
        "        'searchType': 'image',\n",
        "        'num': num_images\n",
        "    }\n",
        "    response = requests.get(search_url, params=params)\n",
        "    results = response.json().get('items', [])\n",
        "    image_urls = [item['link'] for item in results]\n",
        "    return image_urls\n",
        "\n",
        "# Step 10: Example Usage of the Fetch Images Function\n",
        "api_key = 'AIzaSyAU_vOi6G0LeaPbShZst3REbr5C7KNoaTE'\n",
        "search_engine_id = '62245e6dcba8145e3'\n",
        "query = 'french fries'\n",
        "\n",
        "# Fetch a single dynamic image for comparison\n",
        "dynamic_image_url = fetch_images(query, num_images=1, api_key=api_key, search_engine_id=search_engine_id)[0]\n",
        "\n",
        "# Step 11: Function to Download and Preprocess a Single Image\n",
        "def download_and_preprocess_image(url, img_height=150, img_width=150):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        img = img.resize((img_width, img_height))\n",
        "        img_array = np.array(img) / 255.0\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f'Error downloading {url}: {e}')\n",
        "        return None\n",
        "\n",
        "# Step 12: Select a Specific Local Image\n",
        "local_image_path = os.path.join(validation_dir, 'french_fries', '/content/1099260.jpg')  # Example path\n",
        "local_image = Image.open(local_image_path).convert('RGB')\n",
        "local_image_resized = local_image.resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "local_image_array = np.array(local_image_resized) / 255.0\n",
        "\n",
        "# Step 13: Download and Preprocess the Dynamic Image\n",
        "dynamic_image_array = download_and_preprocess_image(dynamic_image_url)\n",
        "\n",
        "# Step 14: Prepare Images for Model Prediction\n",
        "X_compare = np.array([local_image_array, dynamic_image_array])\n",
        "\n",
        "# Step 15: Predict Classes for Both Images\n",
        "predictions = model.predict(X_compare)\n",
        "predicted_class_indices = np.argmax(predictions, axis=1)\n",
        "class_labels = list(train_generator.class_indices.keys())\n",
        "predicted_classes = [class_labels[i] for i in predicted_class_indices]\n",
        "\n",
        "# Step 16: Map Classes to Calorie Estimation\n",
        "calorie_mapping = {\n",
        "    'french_fries': 312,\n",
        "    'samosa': 250,\n",
        "    'pizza': 285\n",
        "}\n",
        "estimated_calories = [calorie_mapping.get(cls, 0) for cls in predicted_classes]\n",
        "\n",
        "# Step 17: Display Local and Dynamic Image Comparison\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Local image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(local_image_resized)\n",
        "plt.title(f'Local Image: {predicted_classes[0]}\\nEstimated Calories: {estimated_calories[0]}')\n",
        "\n",
        "# Dynamic image\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(dynamic_image_array)\n",
        "plt.title(f'Dynamic Image: {predicted_classes[1]}\\nEstimated Calories: {estimated_calories[1]}')\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}